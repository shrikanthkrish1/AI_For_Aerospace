{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c81ae4c-172d-4712-af24-ab430abbca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "# -------------------\n",
    "# 1. Graph Encoder\n",
    "# -------------------\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, node_input_dim, edge_input_dim, hidden_dim):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.node_mlp(x)\n",
    "        edge_attr = self.edge_mlp(edge_attr)\n",
    "        return x, edge_attr\n",
    "\n",
    "# -------------------\n",
    "# 2. Graph Neural Network (Message Passing)\n",
    "# -------------------\n",
    "class GraphNetwork(MessagePassing):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(GraphNetwork, self).__init__(aggr='mean')  # Aggregation method\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        return self.edge_mlp(torch.cat([x_i, x_j, edge_attr], dim=-1))\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.node_mlp(torch.cat([x, aggr_out], dim=-1))\n",
    "\n",
    "# -------------------\n",
    "# 3. Clustering (Graph Pooling)\n",
    "# -------------------\n",
    "class GraphPooling(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(GraphPooling, self).__init__()\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x, clusters):\n",
    "        cluster_embeddings = []\n",
    "        for cluster in clusters:\n",
    "            cluster_x = x[cluster]\n",
    "            cluster_x = cluster_x.unsqueeze(0)  # Add batch dimension\n",
    "            _, h = self.gru(cluster_x)\n",
    "            cluster_embeddings.append(h.squeeze(0))\n",
    "        return torch.stack(cluster_embeddings, dim=0)\n",
    "\n",
    "# -------------------\n",
    "# 4. Transformer (Self-Attention)\n",
    "# -------------------\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = x + self.ffn(x)\n",
    "        return x\n",
    "\n",
    "# -------------------\n",
    "# 5. Decoder (Upsampling and Prediction)\n",
    "# -------------------\n",
    "class GraphDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.gnn = GraphNetwork(hidden_dim)\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "        return self.output_mlp(x)\n",
    "\n",
    "# -------------------\n",
    "# 6. Full Mesh Transformer Architecture\n",
    "# -------------------\n",
    "class MeshTransformer(nn.Module):\n",
    "    def __init__(self, node_input_dim, edge_input_dim, hidden_dim, output_dim, num_heads, num_transformer_layers):\n",
    "        super(MeshTransformer, self).__init__()\n",
    "        self.encoder = GraphEncoder(node_input_dim, edge_input_dim, hidden_dim)\n",
    "        self.gnn = GraphNetwork(hidden_dim)\n",
    "        self.pooling = GraphPooling(hidden_dim)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_dim, num_heads) for _ in range(num_transformer_layers)\n",
    "        ])\n",
    "        self.decoder = GraphDecoder(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, clusters):\n",
    "        # Encode graph\n",
    "        x, edge_attr = self.encoder(x, edge_index, edge_attr)\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        # Pool clusters\n",
    "        cluster_embeddings = self.pooling(x, clusters)\n",
    "\n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            cluster_embeddings = layer(cluster_embeddings)\n",
    "\n",
    "        # Decode back to node-level predictions\n",
    "        return self.decoder(x, edge_index, edge_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a647dcf8-dc90-4afe-9054-2e4c1d18980e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the dimensions\n",
    "node_input_dim = 3    # Example: position, velocity, pressure\n",
    "edge_input_dim = 2    # Example: relative position and distance\n",
    "hidden_dim = 16\n",
    "output_dim = 2        # Example: future velocity and pressure\n",
    "num_heads = 4\n",
    "num_transformer_layers = 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = MeshTransformer(\n",
    "    node_input_dim, edge_input_dim, hidden_dim, output_dim, num_heads, num_transformer_layers\n",
    ")\n",
    "\n",
    "# Create dummy input data\n",
    "num_nodes = 100\n",
    "num_edges = 300\n",
    "num_clusters = 10\n",
    "\n",
    "x = torch.rand((num_nodes, node_input_dim))  # Node features\n",
    "edge_index = torch.randint(0, num_nodes, (2, num_edges))  # Edge index\n",
    "edge_attr = torch.rand((num_edges, edge_input_dim))  # Edge features\n",
    "clusters = [torch.randint(0, num_nodes, (num_nodes // num_clusters,)) for _ in range(num_clusters)]  # Clusters\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(x, edge_index, edge_attr, clusters)\n",
    "\n",
    "# Check the output\n",
    "print(\"Output shape:\", output.shape)  # Should match (num_nodes, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d99bf1-e728-4f1f-9a9d-7c9adf51081f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
